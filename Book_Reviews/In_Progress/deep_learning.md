## Deep Learning
### Introduction to my experience with this book
+ This is an introductory book that is more accessible and provides a high level introduction to complex topics 


### What's different
+ A book that is fully focused on deep learning and ensures new readers to machine learning can keep up with the content
+ More detailed statistical view into machine learning

### What else is good
+

### What can be improved
+

## Brief personal summary of the book and things that I've learned from the book:

+ Chapter 1 - Introduction - Structure of basic deep learning models and motivations

### Part 1 - Applied Math and Machine Learning Basics

+ Chapter 2  - Linear Alegebra - required for deep learning. For deep learning this is around an A level standard and not too difficult.

+ Chapter 3 - Probability and Information Theory - Introduction to basic probability and statistical concepts which spans from A level standards to more complex concepts. 
    + Basic probability concepts include the multinoulli distribution, the Gaussian distribution, Dirac Delta distribution, Gaussian Mixtures and Bayes' rule. 
    + Basic information theory include Shannon information, KL divergence / Cross-entropy, (a commonly used cost for multinoulli distributions).
    + Basic introduction to graphical models - The book misses a focus on the independence assumptions in directed models and considers 'random' to be a sufficient replacement.
    
+ Chapter 4 - Numerical Computation - Low level programming considerations when designing deep learning algorithms as well as the basics of optimisation
    + Underflow and overflow numberical issues
    + Numerical gradient descent using Jacobian and Hessian
    + Surprising amount of detail into constrained optimisation
    
+ Chapter 5 - Machine Learning Basics 
    + Performance, Task, Experience definition of machine learning
    + Bayes error as the best possible correct prediction rate (usually measured with reference to an expert)
    + Detailed comparison of the bias variance problem.
        + Definition of a biased estimator using examples
    + Maximum likelihood and its relation to the Gaussian distribution
    + Dual of an SVM returns alpha, the support vectors themselves
    + k-means clustering as producing a k dimensional one hot vectors
    + Manifolds - Idea of concentrated probability distributions in real life data such as images
        + The idea that most data has discernible pattern in a higher dimension 

### Part 2 - Deep Networks: Modern Practices
    
+ Chapter 6 - Deep Feedforward Networks
    + Acyclic graphs, multiple possible functions due to the feedforward style and width of network
    + Cost function and it's probabilistic interpretation
    + Log(sigmoid) results a scaled softplus function
    
+ Chapter 7 - Regularisation for Deep Learning
+ Chapter 8 - Optimization for Training Deep Models
+ Chapter 9 - Convolutional Networks
+ Chapter 10 - Sequence models: Recurrent and Recursive Nets
+ Chapter 11 - Practical Methodology
+ Chapter 12 - Applications

### Part 3 - Deep Learning Research

+ Chapter 13 - Linear Factor Models
+ Chapter 14 - Autoencoders
+ Chapter 15 - Representation Learning
+ Chapter 16 - Structured Probabilistic Models for Deep Learning
+ Chapter 17 - Monte Carlo Methods
+ Chapter 18 - Confronting the Partition Function
+ Chapter 19 - Monte Carlo Methods
+ Chapter 20 - Confronting the Partition Function

### Things I need to revisit



