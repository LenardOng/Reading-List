# List of books read

* Machine Learning: An Algorithmic Perspective
* Bayesian Artificial Intelligence

# Mini-reviews

## Machine Learning: An Algorithmic Perspective
### What's different
* Useful in giving code that works. Much of machine learning involves writing code to process large amounts of data. Focus on Big-O complexity and providing easy to understand code is very beneficial to the aspiring machine learning enthusiast. 
* The Kalman filter makes a strange appearance alongside expectation maximisation algorithms

### What else is good
* Practical introduction for those wanting to understand the intuition between popular tools such SciKit or Numpy as well as deep learning frameworks like Keras, Tensorflow and Caffe. 
* Inclusion of many different types of dimensionality reduction whilst the most commonly heard ones are just PCA and ICA.
* Exploration vs Exploitation - Basic definitions and a few algorithms. Good introduction to reinforcement learning
* Explanation of various optimization techniques

### What can be improved
*	Maybe more explanation in applications, emphasising the distinction and uses between the four major machine learning problems, dimensionality reduction, regression, classification and clustering.
	
### Things I need to revisit (Not explained too well)
* Viterbi
* Particle Filter
* Hopsfield
* Boltzmann machine
* Contrastive Divergence Learning
	
## Bayesian Artificial Intelligence
### What's different
* Very historical look into the different probability theories
* Very in depth for anyone new to Bayesian idealogy
* Primary focus on Bayesian Networks with many medical examples

### What else is good
* Serves as a decent guide with common mistakes to avoid 
* In depth practical description on the construction of Bayesian Neural Networks
* One of the better explanations on the bias variance trade-off I've read

### What can be improved
* A lot of fluff about the history and evolution of the ideas, not necessarily a bad thing but there could be a larger focus on state-of-the-art algorithms and comparison results to other machine learning techniques
* Information theory is introduced very late in the book, whereas it is an important idea that should be introduced much earlier
    
# In progress

## Pattern Recognition and Machine Learning
### What's different
+ Probably the best book out there by the amount of material is being covered
### What else is good
+ Clearer in explanation on the Information Theory front than McKay
### What can be improved

## Gaussian Processes for Machine Learning
### What's different
### What else is good
+ Bayesian interpretation of common machine learning techniques
### What can be improved

## Convex Optimisation
### What's different
### What else is good
### What can be improved

## Information Theory, Inference and Algorithms
### What's different
+ Large focus on Shannon Information Theory
+ Theory of compression with an emphasis on information theory
### What else is good
+ Very practical with many examples with accompanying solutions to assist learning
+ Clearest definition of probability I've read
### What can be improved
+ Easy to get lost in the maths unless working through exercises
    
## The Elements of Statistical Learning
### What's different
+ This was recommended as a post-graduate level book
+ Very difficult to follow without a strong foundation in the underlying theory
### What else is good
+ Emphasis on the creation of a separating hyperplane
+ Covers regression very quickly and thoroughly with commonly used regularization techniques
### What can be improved
	
## Neural Networks and Learning Machines
### What's different
### What else is good
### What can be improved
+ Not very useful a book for learning as its easy to get lost in the unnecessary maths
	
	